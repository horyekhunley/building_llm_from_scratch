{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6097c873",
   "metadata": {},
   "source": [
    "The most common ways to finetune language models are instruction finetuning and classification finetuning. Instruction finetuning involves training a language model on a set of tasks using specific instructions to improve its ability to understand and execute tasks described in natural language prompts.\n",
    "In classification finetuning, the model is trained to recognize a specific set of class labels such as \"spam\" or \"not spam\". examples go beyond email filtering; they include identifying between different species of plants or animals; categorizing news articles into sports, entertainment, politics etc.\n",
    "Usually an instruction finetuned model can undergo a broader range of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c6d168",
   "metadata": {},
   "source": [
    "Instruction fine-tuning improves a modelâ€™s ability to understand and generate responses\n",
    "based on specific user instructions. Instruction fine-tuning is best suited for models\n",
    "that need to handle a variety of tasks based on complex user instructions, improving\n",
    "flexibility and interaction quality. Classification fine-tuning is ideal for projects requiring precise categorization of data into predefined classes, such as sentiment analysis or spam detection.\n",
    "While instruction fine-tuning is more versatile, it demands larger datasets and greater\n",
    "computational resources to develop models proficient in various tasks. In contrast,\n",
    "classification fine-tuning requires less data and compute power, but its use is confined to the specific classes on which the model has been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60b6aa",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf58149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file already exists at sms_spam_data/SMSSpamCollection.tsv. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extract_dir = \"sms_spam_data\"\n",
    "data_file_path = Path(extract_dir) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "# Download the dataset\n",
    "def download_and_unzip_spam_data(url, zip_path, extract_dir, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"Data file already exists at {data_file_path}. Skipping download.\")\n",
    "        return\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, 'wb') as out_file:\n",
    "            out_file.write(response.read())\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    original_file_path = Path(extract_dir) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"Data downloaded and extracted to {data_file_path}.\")\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extract_dir, data_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78059063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                               Text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv(data_file_path, sep='\\t', header=None, names=['Label', 'Text'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebdc8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b337308",
   "metadata": {},
   "source": [
    "we need to balance the dataset so we can have 747 instances for both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f883ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    num_spam = df[df['Label'] == 'spam'].shape[0]\n",
    "    ham_df = df[df['Label'] == 'ham'].sample(n=num_spam, random_state=123)\n",
    "    balanced_df = pd.concat([ham_df, df[df['Label'] == 'spam']])\n",
    "    return balanced_df\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06da33",
   "metadata": {},
   "source": [
    "next we convert the string class labels into integer class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f00584",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df['Label'] = balanced_df['Label'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd5e4a",
   "metadata": {},
   "source": [
    "next we split out data into training, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed7b37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    \n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)  # Shuffle the DataFrame\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "    \n",
    "    train_df = df.iloc[:train_end]\n",
    "    validation_df = df.iloc[train_end:validation_end]\n",
    "    test_df = df.iloc[validation_end:]\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "\n",
    "train_df.to_csv(\"train_data.csv\", index=None)\n",
    "validation_df.to_csv(\"validation_data.csv\", index=None)\n",
    "test_df.to_csv(\"test_data.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55398e8b",
   "metadata": {},
   "source": [
    "creating dataloaders by padding all messages to the length of the longest message in the dataset or batch. to do this we add padding tokens (<|endoftext|>) to all shorter messages. instead of just adding the string \"<|endoftext|>\" directly, we will add the token id corresponding to \"<|endoftext|>\" to the encoded text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e12df150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# checking the encoding for <|endoftext|>\n",
    "\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})) \n",
    "\n",
    "# the answer is [50256] which is the special token for end of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "517133e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]]\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[: self.max_length] for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "338a3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train_data.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "816aeb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b28eaef",
   "metadata": {},
   "source": [
    "the longest sequence contain no more than 120 tokens which is common for text messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4f9bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation_data.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=train_dataset.max_length\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test_data.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=train_dataset.max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05823e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59c2ddcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([8, 120])\n",
      "Target batch shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# to ensure our dataloaders are working correctly, we iterate over the training loader and then break after the first batch\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "print(\"Target batch shape:\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2897631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "18 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d20a26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True,\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a41c9392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "import sys\n",
    "import os\n",
    "\n",
    "ch04_path = os.path.abspath(\"LLMs-from-scratch/ch04/01_main-chapter-code\")\n",
    "\n",
    "if ch04_path not in sys.path:\n",
    "    sys.path.append(ch04_path)\n",
    "from gpt import GPTModel\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "model.load_state_dict(load_file(\"./gpt2/124M/gpt2-small-124M.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a197055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 209 keys; showing first 200:\n",
      "000: final_norm.scale\n",
      "001: final_norm.shift\n",
      "002: out_head.weight\n",
      "003: pos_emb.weight\n",
      "004: tok_emb.weight\n",
      "005: trf_blocks.0.att.W_key.bias\n",
      "006: trf_blocks.0.att.W_key.weight\n",
      "007: trf_blocks.0.att.W_query.bias\n",
      "008: trf_blocks.0.att.W_query.weight\n",
      "009: trf_blocks.0.att.W_value.bias\n",
      "010: trf_blocks.0.att.W_value.weight\n",
      "011: trf_blocks.0.att.mask\n",
      "012: trf_blocks.0.att.out_proj.bias\n",
      "013: trf_blocks.0.att.out_proj.weight\n",
      "014: trf_blocks.0.ff.layers.0.bias\n",
      "015: trf_blocks.0.ff.layers.0.weight\n",
      "016: trf_blocks.0.ff.layers.2.bias\n",
      "017: trf_blocks.0.ff.layers.2.weight\n",
      "018: trf_blocks.0.norm1.scale\n",
      "019: trf_blocks.0.norm1.shift\n",
      "020: trf_blocks.0.norm2.scale\n",
      "021: trf_blocks.0.norm2.shift\n",
      "022: trf_blocks.1.att.W_key.bias\n",
      "023: trf_blocks.1.att.W_key.weight\n",
      "024: trf_blocks.1.att.W_query.bias\n",
      "025: trf_blocks.1.att.W_query.weight\n",
      "026: trf_blocks.1.att.W_value.bias\n",
      "027: trf_blocks.1.att.W_value.weight\n",
      "028: trf_blocks.1.att.mask\n",
      "029: trf_blocks.1.att.out_proj.bias\n",
      "030: trf_blocks.1.att.out_proj.weight\n",
      "031: trf_blocks.1.ff.layers.0.bias\n",
      "032: trf_blocks.1.ff.layers.0.weight\n",
      "033: trf_blocks.1.ff.layers.2.bias\n",
      "034: trf_blocks.1.ff.layers.2.weight\n",
      "035: trf_blocks.1.norm1.scale\n",
      "036: trf_blocks.1.norm1.shift\n",
      "037: trf_blocks.1.norm2.scale\n",
      "038: trf_blocks.1.norm2.shift\n",
      "039: trf_blocks.10.att.W_key.bias\n",
      "040: trf_blocks.10.att.W_key.weight\n",
      "041: trf_blocks.10.att.W_query.bias\n",
      "042: trf_blocks.10.att.W_query.weight\n",
      "043: trf_blocks.10.att.W_value.bias\n",
      "044: trf_blocks.10.att.W_value.weight\n",
      "045: trf_blocks.10.att.mask\n",
      "046: trf_blocks.10.att.out_proj.bias\n",
      "047: trf_blocks.10.att.out_proj.weight\n",
      "048: trf_blocks.10.ff.layers.0.bias\n",
      "049: trf_blocks.10.ff.layers.0.weight\n",
      "050: trf_blocks.10.ff.layers.2.bias\n",
      "051: trf_blocks.10.ff.layers.2.weight\n",
      "052: trf_blocks.10.norm1.scale\n",
      "053: trf_blocks.10.norm1.shift\n",
      "054: trf_blocks.10.norm2.scale\n",
      "055: trf_blocks.10.norm2.shift\n",
      "056: trf_blocks.11.att.W_key.bias\n",
      "057: trf_blocks.11.att.W_key.weight\n",
      "058: trf_blocks.11.att.W_query.bias\n",
      "059: trf_blocks.11.att.W_query.weight\n",
      "060: trf_blocks.11.att.W_value.bias\n",
      "061: trf_blocks.11.att.W_value.weight\n",
      "062: trf_blocks.11.att.mask\n",
      "063: trf_blocks.11.att.out_proj.bias\n",
      "064: trf_blocks.11.att.out_proj.weight\n",
      "065: trf_blocks.11.ff.layers.0.bias\n",
      "066: trf_blocks.11.ff.layers.0.weight\n",
      "067: trf_blocks.11.ff.layers.2.bias\n",
      "068: trf_blocks.11.ff.layers.2.weight\n",
      "069: trf_blocks.11.norm1.scale\n",
      "070: trf_blocks.11.norm1.shift\n",
      "071: trf_blocks.11.norm2.scale\n",
      "072: trf_blocks.11.norm2.shift\n",
      "073: trf_blocks.2.att.W_key.bias\n",
      "074: trf_blocks.2.att.W_key.weight\n",
      "075: trf_blocks.2.att.W_query.bias\n",
      "076: trf_blocks.2.att.W_query.weight\n",
      "077: trf_blocks.2.att.W_value.bias\n",
      "078: trf_blocks.2.att.W_value.weight\n",
      "079: trf_blocks.2.att.mask\n",
      "080: trf_blocks.2.att.out_proj.bias\n",
      "081: trf_blocks.2.att.out_proj.weight\n",
      "082: trf_blocks.2.ff.layers.0.bias\n",
      "083: trf_blocks.2.ff.layers.0.weight\n",
      "084: trf_blocks.2.ff.layers.2.bias\n",
      "085: trf_blocks.2.ff.layers.2.weight\n",
      "086: trf_blocks.2.norm1.scale\n",
      "087: trf_blocks.2.norm1.shift\n",
      "088: trf_blocks.2.norm2.scale\n",
      "089: trf_blocks.2.norm2.shift\n",
      "090: trf_blocks.3.att.W_key.bias\n",
      "091: trf_blocks.3.att.W_key.weight\n",
      "092: trf_blocks.3.att.W_query.bias\n",
      "093: trf_blocks.3.att.W_query.weight\n",
      "094: trf_blocks.3.att.W_value.bias\n",
      "095: trf_blocks.3.att.W_value.weight\n",
      "096: trf_blocks.3.att.mask\n",
      "097: trf_blocks.3.att.out_proj.bias\n",
      "098: trf_blocks.3.att.out_proj.weight\n",
      "099: trf_blocks.3.ff.layers.0.bias\n",
      "100: trf_blocks.3.ff.layers.0.weight\n",
      "101: trf_blocks.3.ff.layers.2.bias\n",
      "102: trf_blocks.3.ff.layers.2.weight\n",
      "103: trf_blocks.3.norm1.scale\n",
      "104: trf_blocks.3.norm1.shift\n",
      "105: trf_blocks.3.norm2.scale\n",
      "106: trf_blocks.3.norm2.shift\n",
      "107: trf_blocks.4.att.W_key.bias\n",
      "108: trf_blocks.4.att.W_key.weight\n",
      "109: trf_blocks.4.att.W_query.bias\n",
      "110: trf_blocks.4.att.W_query.weight\n",
      "111: trf_blocks.4.att.W_value.bias\n",
      "112: trf_blocks.4.att.W_value.weight\n",
      "113: trf_blocks.4.att.mask\n",
      "114: trf_blocks.4.att.out_proj.bias\n",
      "115: trf_blocks.4.att.out_proj.weight\n",
      "116: trf_blocks.4.ff.layers.0.bias\n",
      "117: trf_blocks.4.ff.layers.0.weight\n",
      "118: trf_blocks.4.ff.layers.2.bias\n",
      "119: trf_blocks.4.ff.layers.2.weight\n",
      "120: trf_blocks.4.norm1.scale\n",
      "121: trf_blocks.4.norm1.shift\n",
      "122: trf_blocks.4.norm2.scale\n",
      "123: trf_blocks.4.norm2.shift\n",
      "124: trf_blocks.5.att.W_key.bias\n",
      "125: trf_blocks.5.att.W_key.weight\n",
      "126: trf_blocks.5.att.W_query.bias\n",
      "127: trf_blocks.5.att.W_query.weight\n",
      "128: trf_blocks.5.att.W_value.bias\n",
      "129: trf_blocks.5.att.W_value.weight\n",
      "130: trf_blocks.5.att.mask\n",
      "131: trf_blocks.5.att.out_proj.bias\n",
      "132: trf_blocks.5.att.out_proj.weight\n",
      "133: trf_blocks.5.ff.layers.0.bias\n",
      "134: trf_blocks.5.ff.layers.0.weight\n",
      "135: trf_blocks.5.ff.layers.2.bias\n",
      "136: trf_blocks.5.ff.layers.2.weight\n",
      "137: trf_blocks.5.norm1.scale\n",
      "138: trf_blocks.5.norm1.shift\n",
      "139: trf_blocks.5.norm2.scale\n",
      "140: trf_blocks.5.norm2.shift\n",
      "141: trf_blocks.6.att.W_key.bias\n",
      "142: trf_blocks.6.att.W_key.weight\n",
      "143: trf_blocks.6.att.W_query.bias\n",
      "144: trf_blocks.6.att.W_query.weight\n",
      "145: trf_blocks.6.att.W_value.bias\n",
      "146: trf_blocks.6.att.W_value.weight\n",
      "147: trf_blocks.6.att.mask\n",
      "148: trf_blocks.6.att.out_proj.bias\n",
      "149: trf_blocks.6.att.out_proj.weight\n",
      "150: trf_blocks.6.ff.layers.0.bias\n",
      "151: trf_blocks.6.ff.layers.0.weight\n",
      "152: trf_blocks.6.ff.layers.2.bias\n",
      "153: trf_blocks.6.ff.layers.2.weight\n",
      "154: trf_blocks.6.norm1.scale\n",
      "155: trf_blocks.6.norm1.shift\n",
      "156: trf_blocks.6.norm2.scale\n",
      "157: trf_blocks.6.norm2.shift\n",
      "158: trf_blocks.7.att.W_key.bias\n",
      "159: trf_blocks.7.att.W_key.weight\n",
      "160: trf_blocks.7.att.W_query.bias\n",
      "161: trf_blocks.7.att.W_query.weight\n",
      "162: trf_blocks.7.att.W_value.bias\n",
      "163: trf_blocks.7.att.W_value.weight\n",
      "164: trf_blocks.7.att.mask\n",
      "165: trf_blocks.7.att.out_proj.bias\n",
      "166: trf_blocks.7.att.out_proj.weight\n",
      "167: trf_blocks.7.ff.layers.0.bias\n",
      "168: trf_blocks.7.ff.layers.0.weight\n",
      "169: trf_blocks.7.ff.layers.2.bias\n",
      "170: trf_blocks.7.ff.layers.2.weight\n",
      "171: trf_blocks.7.norm1.scale\n",
      "172: trf_blocks.7.norm1.shift\n",
      "173: trf_blocks.7.norm2.scale\n",
      "174: trf_blocks.7.norm2.shift\n",
      "175: trf_blocks.8.att.W_key.bias\n",
      "176: trf_blocks.8.att.W_key.weight\n",
      "177: trf_blocks.8.att.W_query.bias\n",
      "178: trf_blocks.8.att.W_query.weight\n",
      "179: trf_blocks.8.att.W_value.bias\n",
      "180: trf_blocks.8.att.W_value.weight\n",
      "181: trf_blocks.8.att.mask\n",
      "182: trf_blocks.8.att.out_proj.bias\n",
      "183: trf_blocks.8.att.out_proj.weight\n",
      "184: trf_blocks.8.ff.layers.0.bias\n",
      "185: trf_blocks.8.ff.layers.0.weight\n",
      "186: trf_blocks.8.ff.layers.2.bias\n",
      "187: trf_blocks.8.ff.layers.2.weight\n",
      "188: trf_blocks.8.norm1.scale\n",
      "189: trf_blocks.8.norm1.shift\n",
      "190: trf_blocks.8.norm2.scale\n",
      "191: trf_blocks.8.norm2.shift\n",
      "192: trf_blocks.9.att.W_key.bias\n",
      "193: trf_blocks.9.att.W_key.weight\n",
      "194: trf_blocks.9.att.W_query.bias\n",
      "195: trf_blocks.9.att.W_query.weight\n",
      "196: trf_blocks.9.att.W_value.bias\n",
      "197: trf_blocks.9.att.W_value.weight\n",
      "198: trf_blocks.9.att.mask\n",
      "199: trf_blocks.9.att.out_proj.bias\n",
      "Top-level matches:\n",
      " wte -> None\n",
      " wpe -> pos_emb.weight\n",
      " ln_f.weight -> None\n",
      " ln_f.bias -> None\n",
      "Missing top-level mappings after heuristic: ['wte', 'g', 'b']\n",
      "Scan the printed keys above for likely candidates and adjust search patterns.\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "import numpy as np, torch, re\n",
    "from pathlib import Path\n",
    "from llms_from_scratch.ch05 import load_weights_into_gpt\n",
    "\n",
    "model_path = \"./gpt2/124M/gpt2-small-124M.safetensors\"\n",
    "params_flat = load_file(model_path)\n",
    "keys = list(params_flat.keys())\n",
    "print(f\"Loaded {len(keys)} keys; showing first 200:\")\n",
    "for i,k in enumerate(keys[:200]): print(f\"{i:03d}: {k}\")\n",
    "\n",
    "def to_numpy(val):\n",
    "    return val.cpu().numpy() if isinstance(val, torch.Tensor) else np.array(val)\n",
    "\n",
    "def find_key(params, candidates):\n",
    "    # return first key whose name contains any candidate substring\n",
    "    for k in params:\n",
    "        for c in candidates:\n",
    "            if c in k:\n",
    "                return k, to_numpy(params[k])\n",
    "    return None, None\n",
    "\n",
    "# Top-level mappings (common candidates)\n",
    "wte_k, wte = find_key(params_flat, [\"wte\", \"word_embeddings\", \"lm_head.weight\", \"transformer.wte\", \"embeddings.word_embeddings\"])\n",
    "wpe_k, wpe = find_key(params_flat, [\"wpe\", \"position_embeddings\", \"pos_emb\", \"transformer.wpe\"])\n",
    "g_k, g = find_key(params_flat, [\"ln_f.weight\", \"transformer.ln_f\", \"final_layernorm.weight\", \"ln_f\"])\n",
    "b_k, b = find_key(params_flat, [\"ln_f.bias\", \"transformer.ln_f.bias\", \"final_layernorm.bias\", \"ln_f.b\"])\n",
    "\n",
    "print(\"Top-level matches:\")\n",
    "print(\" wte ->\", wte_k)\n",
    "print(\" wpe ->\", wpe_k)\n",
    "print(\" ln_f.weight ->\", g_k)\n",
    "print(\" ln_f.bias ->\", b_k)\n",
    "\n",
    "# assemble blocks\n",
    "n_layers = len(model.trf_blocks)\n",
    "blocks = [None]*n_layers\n",
    "\n",
    "# helper to find per-layer keys (tries multiple naming schemes)\n",
    "def find_layer_param(params, layer, tail_variants):\n",
    "    candidates = []\n",
    "    prefixes = [f\"transformer.h.{layer}.\", f\"model.h.{layer}.\", f\"h.{layer}.\", f\"blocks.{layer}.\", f\"{layer}.\"]\n",
    "    for p in prefixes:\n",
    "        for t in tail_variants:\n",
    "            candidates.append(p + t)\n",
    "            candidates.append(p + t + \".weight\")\n",
    "            candidates.append(p + t + \".bias\")\n",
    "            candidates.append(p + t + \".w\")\n",
    "            candidates.append(p + t + \".b\")\n",
    "    # also try substring match\n",
    "    for c in candidates:\n",
    "        for k in params:\n",
    "            if k.endswith(c) or (c in k and not k.endswith(\".meta\")):\n",
    "                return k, to_numpy(params[k])\n",
    "    # fallback: substring search\n",
    "    for t in tail_variants:\n",
    "        for k in params:\n",
    "            if f\".{layer}.\" in k and t in k:\n",
    "                return k, to_numpy(params[k])\n",
    "    return None, None\n",
    "\n",
    "for i in range(n_layers):\n",
    "    blk = {\"attn\":{}, \"mlp\":{}, \"ln_1\":{}, \"ln_2\":{}}\n",
    "    # combined qkv (sometimes named c_attn or attn.c_attn or c_attn.weight)\n",
    "    kq_w, q_w = find_layer_param(params_flat, i, [\"attn.c_attn\", \"c_attn\", \"attn.c_attn.weight\", \"attn/c_attn\"])\n",
    "    kb_w, qb = find_layer_param(params_flat, i, [\"attn.c_attn.bias\", \"c_attn.bias\", \"c_attn.b\"])\n",
    "    if q_w is not None:\n",
    "        blk[\"attn\"][\"c_attn\"] = {\"w\": q_w, \"b\": qb}\n",
    "    # c_proj / out_proj\n",
    "    kproj_w, proj_w = find_layer_param(params_flat, i, [\"attn.c_proj\", \"c_proj\", \"attn.c_proj.weight\"])\n",
    "    kproj_b, proj_b = find_layer_param(params_flat, i, [\"attn.c_proj.bias\", \"c_proj.bias\"])\n",
    "    if proj_w is not None:\n",
    "        blk[\"attn\"][\"c_proj\"] = {\"w\": proj_w, \"b\": proj_b}\n",
    "    # mlp\n",
    "    kfc_w, fc_w = find_layer_param(params_flat, i, [\"mlp.c_fc\", \"mlp.c_fc.weight\", \"c_fc\"])\n",
    "    kfc_b, fc_b = find_layer_param(params_flat, i, [\"mlp.c_fc.bias\", \"c_fc.bias\"])\n",
    "    kproj2_w, proj2_w = find_layer_param(params_flat, i, [\"mlp.c_proj\", \"mlp.c_proj.weight\", \"c_proj\"])\n",
    "    kproj2_b, proj2_b = find_layer_param(params_flat, i, [\"mlp.c_proj.bias\", \"c_proj.bias\"])\n",
    "    if fc_w is not None: blk[\"mlp\"][\"c_fc\"] = {\"w\": fc_w, \"b\": fc_b}\n",
    "    if proj2_w is not None: blk[\"mlp\"][\"c_proj\"] = {\"w\": proj2_w, \"b\": proj2_b}\n",
    "    # layer norms\n",
    "    ln1_g_k, ln1_g = find_layer_param(params_flat, i, [\"ln_1.weight\", \"ln_1.gamma\", \"ln_1.g\"])\n",
    "    ln1_b_k, ln1_b = find_layer_param(params_flat, i, [\"ln_1.bias\", \"ln_1.beta\", \"ln_1.b\"])\n",
    "    ln2_g_k, ln2_g = find_layer_param(params_flat, i, [\"ln_2.weight\", \"ln_2.gamma\", \"ln_2.g\"])\n",
    "    ln2_b_k, ln2_b = find_layer_param(params_flat, i, [\"ln_2.bias\", \"ln_2.beta\", \"ln_2.b\"])\n",
    "    if ln1_g is not None: blk[\"ln_1\"][\"g\"] = ln1_g\n",
    "    if ln1_b is not None: blk[\"ln_1\"][\"b\"] = ln1_b\n",
    "    if ln2_g is not None: blk[\"ln_2\"][\"g\"] = ln2_g\n",
    "    if ln2_b is not None: blk[\"ln_2\"][\"b\"] = ln2_b\n",
    "    blocks[i] = blk\n",
    "\n",
    "params_struct = {\"wte\": wte, \"wpe\": wpe, \"blocks\": blocks, \"g\": g, \"b\": b}\n",
    "\n",
    "# diagnostics: which top-level entries are still None\n",
    "miss = [k for k,v in params_struct.items() if v is None]\n",
    "if miss:\n",
    "    print(\"Missing top-level mappings after heuristic:\", miss)\n",
    "    print(\"Scan the printed keys above for likely candidates and adjust search patterns.\")\n",
    "else:\n",
    "    print(\"Top-level mappings found. Attempting to load into model...\")\n",
    "    load_weights_into_gpt(model, params_struct)\n",
    "    model.eval()\n",
    "    print(\"Weights loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cf645fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_path = os.path.abspath(\"LLMs-from-scratch/pkg\")\n",
    "\n",
    "# Add to sys.path if not already present\n",
    "if pkg_path not in sys.path:\n",
    "    sys.path.append(pkg_path)\n",
    "from llms_from_scratch.ch05 import load_weights_into_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2455207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "from llms_from_scratch.ch04 import generate_text_simple\n",
    "from llms_from_scratch.ch05 import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "37f94502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': You are a winner you have been specially selected to receive a $1000 Walmart gift card. Click here to claim now.!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no': \"\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive a $1000 Walmart gift card. Click here to claim now.\"\n",
    ")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f512a",
   "metadata": {},
   "source": [
    "the model is currently struggling to follow instructions, so now we need to do more work to prepare the model for classification finetuning by adding a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd70e267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68714cbd",
   "metadata": {},
   "source": [
    "to get the model ready for finetuning, we first freeze the model meaning we make all layers untrainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9fc7eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd57c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ff3570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68423b98",
   "metadata": {},
   "source": [
    "Calculating the classification loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd9d690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    \n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += ((predicted_labels == target_batch).sum().item())\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9052252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy (first 10 batches): 53.75%\n",
      "Validation accuracy (first 10 batches): 55.00%\n",
      "Test accuracy (first 10 batches): 51.25%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "print(f\"Train accuracy (first 10 batches): {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy (first 10 batches): {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy (first 10 batches): {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c79b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
